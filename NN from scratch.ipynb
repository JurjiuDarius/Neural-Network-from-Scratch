{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "ba654ef7-0f9e-4ffb-b404-bb38d89e7972",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(784, 41000)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "data=np.array(pd.read_csv(\"images.csv\"))\n",
        "np.random.shuffle(data)\n",
        "m,n=data.shape\n",
        "\n",
        "#Extracting the labels\n",
        "X_train=data[0:m-1000,1:].T\n",
        "Y_train=data[0:m-1000,0]\n",
        "\n",
        "X_test=data[m-1000:m,1:].T\n",
        "Y_test=data[m-1000:m,0]\n",
        "\n",
        "X_train=X_train/255\n",
        "X_test=X_test/255\n",
        "\n",
        "X_train.shape\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96c039e-df51-4094-beea-5eec3b647e60",
      "metadata": {},
      "source": [
        "#### The following is an implementation of a neural network with one hidden layer. This is meant to help get a better feel for how the differentiation works. All the formulas can be deduced from the math in the following markdown cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "71edb76d-c928-448e-aee5-7fcc173075a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_params():\n",
        "    W1 = np.random.rand(20, 784) - 0.5\n",
        "    b1 = np.random.rand(20, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 20) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def reLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "    \n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = reLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def reLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y):\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    #This is the derivative of cross-entropy. Implementing the actual loss function is only necessary for metrics.\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)\n",
        "    dZ1 = W2.T.dot(dZ2) * reLU_deriv(Z1)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1    \n",
        "    W2 = W2 - alpha * dW2  \n",
        "    b2 = b2 - alpha * db2    \n",
        "    return W1, b1, W2, b2            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "36c39140-9862-4541-ab34-ab0fcaa622be",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "0.07563414634146341\n",
            "Epoch:  10\n",
            "0.2983170731707317\n",
            "Epoch:  20\n",
            "0.4287073170731707\n",
            "Epoch:  30\n",
            "0.5088292682926829\n",
            "Epoch:  40\n",
            "0.5664878048780487\n",
            "Epoch:  50\n",
            "0.6084878048780488\n",
            "Epoch:  60\n",
            "0.6422926829268293\n",
            "Epoch:  70\n",
            "0.6700487804878049\n",
            "Epoch:  80\n",
            "0.692560975609756\n",
            "Epoch:  90\n",
            "0.7105853658536585\n",
            "Epoch:  100\n",
            "0.725780487804878\n",
            "Epoch:  110\n",
            "0.7395121951219512\n",
            "Epoch:  120\n",
            "0.7498536585365854\n",
            "Epoch:  130\n",
            "0.7594146341463415\n",
            "Epoch:  140\n",
            "0.7686585365853659\n",
            "Epoch:  150\n",
            "0.7768780487804878\n",
            "Epoch:  160\n",
            "0.7827804878048781\n",
            "Epoch:  170\n",
            "0.7896585365853659\n",
            "Epoch:  180\n",
            "0.7958048780487805\n",
            "Epoch:  190\n",
            "0.801829268292683\n",
            "Epoch:  200\n",
            "0.806609756097561\n",
            "Epoch:  210\n",
            "0.8108536585365854\n",
            "Epoch:  220\n",
            "0.8150975609756097\n",
            "Epoch:  230\n",
            "0.8190731707317073\n",
            "Epoch:  240\n",
            "0.8227560975609756\n",
            "Epoch:  250\n",
            "0.8253414634146341\n",
            "Epoch:  260\n",
            "0.8284878048780487\n",
            "Epoch:  270\n",
            "0.8313170731707317\n",
            "Epoch:  280\n",
            "0.8335609756097561\n",
            "Epoch:  290\n",
            "0.8364146341463414\n",
            "Epoch:  300\n",
            "0.8390731707317073\n",
            "Epoch:  310\n",
            "0.8415853658536585\n",
            "Epoch:  320\n",
            "0.8441463414634146\n",
            "Epoch:  330\n",
            "0.8460487804878049\n",
            "Epoch:  340\n",
            "0.8480487804878049\n",
            "Epoch:  350\n",
            "0.850170731707317\n",
            "Epoch:  360\n",
            "0.8519268292682927\n",
            "Epoch:  370\n",
            "0.8537560975609756\n",
            "Epoch:  380\n",
            "0.8556585365853658\n",
            "Epoch:  390\n",
            "0.8574146341463414\n",
            "Epoch:  400\n",
            "0.8590975609756097\n",
            "Epoch:  410\n",
            "0.8604390243902439\n",
            "Epoch:  420\n",
            "0.862\n",
            "Epoch:  430\n",
            "0.8634390243902439\n",
            "Epoch:  440\n",
            "0.8648536585365854\n",
            "Epoch:  450\n",
            "0.8664390243902439\n",
            "Epoch:  460\n",
            "0.8678292682926829\n",
            "Epoch:  470\n",
            "0.8688536585365854\n",
            "Epoch:  480\n",
            "0.87\n",
            "Epoch:  490\n",
            "0.8707560975609756\n"
          ]
        }
      ],
      "source": [
        "def get_predictions(A):\n",
        "    return np.argmax(A, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def grad_descent(X, Y, alpha, epochs):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(epochs):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 10 == 0:\n",
        "            print(\"Epoch: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "W1, b1, W2, b2 = grad_descent(X_train, Y_train, 0.10, 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97d643a-8e22-44ac-8c75-952fe6db354a",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### **This is a nice way of getting a better feel for how the derivatives work, but a generalizable implementation is needed.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02035806-0481-4fa1-96bb-2ed5a5581927",
      "metadata": {},
      "source": [
        "\n",
        "#### Here's the math for the backpropagation process. The idea is to basically view each layer as its own, optimizable unit which only needs the derivative of the previous layer to optimize the parameters\n",
        "\n",
        "#### $\\frac{\\partial L}{\\partial z_n} = A_n-Y$, \n",
        "##### where z_n represents the values passed to the activation in the last layer and A_n are the \"activated\" values\n",
        "\n",
        "#### $ \\frac{\\partial L}{\\partial z_{i-1}} = \\frac{\\partial L}{\\partial z_i}*\\frac{\\partial z_i}{\\partial z_{i-1}}$\n",
        "\n",
        "#### $ \\frac{\\partial z_i}{\\partial z_{i-1}} = w_i*\\frac{\\partial A_{i-1}}{\\partial z_{i-1}} $\n",
        "\n",
        "#### $ \\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial z_i}*\\frac{\\partial z_i}{\\partial w_i} = \\frac{\\partial L}{\\partial z_i} * A_{i-1}^T $\n",
        "\n",
        "#### $ \\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial z_i}*\\frac{\\partial z_i}{\\partial b_i} = {\\frac{\\partial L}{\\partial z_{i}}}$\n",
        "\n",
        "\n",
        "#### This is an inductive approach which makes it feasable to assemble multiple layers in a single network. The implementation is given below. I have also implemented the sigmoid function, but reLU works better here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "77bf2526-8eea-47d2-b6a1-f29a8ef2c3c9",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "Accuracy 0.085\n",
            "Epoch:  10\n",
            "Accuracy 0.221\n",
            "Epoch:  20\n",
            "Accuracy 0.361\n",
            "Epoch:  30\n",
            "Accuracy 0.447\n",
            "Epoch:  40\n",
            "Accuracy 0.506\n",
            "Epoch:  50\n",
            "Accuracy 0.577\n",
            "Epoch:  60\n",
            "Accuracy 0.614\n",
            "Epoch:  70\n",
            "Accuracy 0.647\n",
            "Epoch:  80\n",
            "Accuracy 0.672\n",
            "Epoch:  90\n",
            "Accuracy 0.708\n",
            "Epoch:  100\n",
            "Accuracy 0.723\n",
            "Epoch:  110\n",
            "Accuracy 0.738\n",
            "Epoch:  120\n",
            "Accuracy 0.751\n",
            "Epoch:  130\n",
            "Accuracy 0.763\n",
            "Epoch:  140\n",
            "Accuracy 0.776\n",
            "Epoch:  150\n",
            "Accuracy 0.791\n",
            "Epoch:  160\n",
            "Accuracy 0.797\n",
            "Epoch:  170\n",
            "Accuracy 0.804\n",
            "Epoch:  180\n",
            "Accuracy 0.81\n",
            "Epoch:  190\n",
            "Accuracy 0.813\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def cross_entropy(Z,Y):\n",
        "    return -Z*np.log(Y)\n",
        "\n",
        "def cross_entropy_deriv(A,Y):\n",
        "    return A-Y\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def sigmoid_deriv(Z):\n",
        "    return sigmoid(Z)*(1-sigmoid(Z))\n",
        "\n",
        "derivatives={reLU:reLU_deriv, cross_entropy:cross_entropy_deriv,sigmoid:sigmoid_deriv}\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self,input_size,nr_neurons,activation,is_first):\n",
        "        self.activation=activation\n",
        "        self.weights=np.random.rand(nr_neurons,input_size)-0.5\n",
        "        self.biases=np.random.rand(nr_neurons,1)-0.5\n",
        "        self.is_first=is_first\n",
        "        \n",
        "    def forward_pass(self,input):\n",
        "        return self.weights.dot(input)+self.biases, self.activation(self.weights.dot(input)+self.biases)\n",
        "    \n",
        "    def backprop(self,dz,z_value,activation,batch_size,learning_rate):\n",
        "        dweights=dz.dot(activation.T)/batch_size\n",
        "        dbiases=dz.sum(axis=1,keepdims=True)/batch_size\n",
        "        self.weights=self.weights-learning_rate*dweights\n",
        "        self.biases=self.biases-learning_rate*dbiases\n",
        "        next_dz=self.weights.T.dot(dz)*derivatives[reLU](z_value)\n",
        "        return next_dz\n",
        "        \n",
        "        \n",
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self,loss=cross_entropy):\n",
        "        self.layers=[]\n",
        "        self.loss=loss\n",
        "    \n",
        "    def train(self,batch,labels,learning_rate):\n",
        "        z_values=[]\n",
        "        activation_values=[]\n",
        "        temp_activations=batch\n",
        "        #This is only here to make the dimensions match when calculating the dz derivative. The last dz is never used and \n",
        "        #its value refers to the actual input data, which obviously can't be optimized \n",
        "        z_values.append(batch)\n",
        "        activation_values.append(batch)\n",
        "        for layer in self.layers:\n",
        "            z,a=layer.forward_pass(temp_activations)\n",
        "            z_values.append(z)\n",
        "            activation_values.append(a)\n",
        "            temp_activations=a\n",
        "        one_hot_y=one_hot(labels)\n",
        "        dz=derivatives[self.loss](temp_activations,one_hot_y)\n",
        "        z_values.pop()\n",
        "        activation_values.pop()\n",
        "        for i in range(len(z_values)-1,-1,-1):\n",
        "            dz=self.layers[i].backprop(dz,z_values[i],activation_values[i],batch.shape[1],learning_rate)\n",
        "            \n",
        "    def add_layer(self,input_size,nr_neurons,activation):\n",
        "        is_first= len(self.layers)==0\n",
        "        self.layers.append(DenseLayer(input_size,nr_neurons,activation,is_first))\n",
        "    \n",
        "    def predict(self,input):\n",
        "        temp=input\n",
        "        for layer in self.layers:\n",
        "            temp=layer.forward_pass(temp)[1]\n",
        "        return np.argmax(temp,0)\n",
        "    \n",
        "EPOCHS=200\n",
        "nn=NeuralNetwork()\n",
        "nn.add_layer(784,30,reLU)\n",
        "nn.add_layer(30,20,reLU)\n",
        "nn.add_layer(20,10,softmax)\n",
        "for i in range(EPOCHS):\n",
        "\n",
        "    nn.train(X_train,Y_train,0.1)\n",
        "    if i % 10 == 0:\n",
        "        predictions=nn.predict(X_test)\n",
        "        print(\"Epoch: \", i)\n",
        "        print(\"Accuracy\",get_accuracy(predictions,Y_test))  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
